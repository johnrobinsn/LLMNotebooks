{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RycWj6ypz4Ym"
      },
      "source": [
        "# Tiny ChatGPT\n",
        "by John Robinson 02/28/2023 [Follow @johnrobinsn on Twitter](https://twitter.com/johnrobinsn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://www.storminthecastle.com/img/01_tiny_chatgpt_files/surfin.png\"><br/>\n",
        "\n",
        "[<img src=\"https://www.storminthecastle.com/img/github.svg\">](https://github.com/johnrobinsn/LLMNotebooks/blob/main/nbs/01_tiny_chatgpt.ipynb) [<img src=\"https://www.storminthecastle.com/img/colab.svg\">](https://colab.research.google.com/github/johnrobinsn/LLMNotebooks/blob/main/nbs/01_tiny_chatgpt.ipynb)\n",
        "\n",
        "Playing around a little bit with langchain and \"chain of thought reasoning\" using gpt-3.  Using a prompt to guide gpt-3 to answer a fairly complex chain of related questions.  This doesn't use \"ChatGPT\" but rather a combination of langchain memory and the gpt-3 model from Open AI.  You can see the results below.\n",
        "\n",
        "You can easily try this out in colab for yourselves.  You'll need a [key from OpenAI](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key).  That you can paste securely into the prompt below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qivAwXD-sSJa"
      },
      "outputs": [],
      "source": [
        "def install_dependencies():\n",
        "  !pip install langchain\n",
        "  !pip install openai\n",
        "\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pEP04XUiqKdy"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3gDecPg1BIy"
      },
      "source": [
        "As mentioned earlier, you'll need to provide an API key from OpenAI since this note book uses their gpt-3 service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbx4HCcRr57q"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "secret = getpass('Enter the secret value: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWMZeIFSrsuI"
      },
      "outputs": [],
      "source": [
        "# stuff the api key into a local environment variable so that the OpenAI module can access it\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY']= secret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njo9ZzjZ1nKK"
      },
      "source": [
        "A prompt template that will feed in the history (memory) of the chat session into the LLM for it to use as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FlMw4BcbrY_v"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3KFBo0152r"
      },
      "source": [
        "A little class wrapper around an LLMChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1NqofCP-rkuF"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self):\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"history\", \"human_input\"], \n",
        "            template=template)\n",
        "\n",
        "        self.model = LLMChain(            \n",
        "            prompt=prompt,\n",
        "            llm=OpenAI(temperature=0,max_tokens=2000),\n",
        "            #verbose=True,\n",
        "            memory = ConversationBufferMemory(),\n",
        "        )\n",
        "\n",
        "    def predict(self, human_input):\n",
        "        return self.model.predict(human_input=human_input)\n",
        "\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj4ZG34L2BCE"
      },
      "source": [
        "Now that the model is all setup we can try asking it a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sjX7G2ZCtPyO",
        "outputId": "3e4455ee-2e7b-4eb0-f2e2-da408980b757"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Neil Armstrong was the first man to walk on the moon on July 20, 1969.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(\"Who was the first man to walk on the moon?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuBEkKX72Gp1"
      },
      "source": [
        "## Chain of Thought Reasoning Example\n",
        "\n",
        "Now we'll try a much more complex interaction consisting of a chain of questions that can refer to earlier answers.  We guide this interpretation within the prompt itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eK-wBVuxtZeT"
      },
      "outputs": [],
      "source": [
        "question_chain = '''\n",
        "Answer the following questions, which refer to the answers of previous questions or to things you already know or understand.\n",
        "\n",
        "Q1.  Who was the first U.S. President?\n",
        "Q2. How many biological children did A1 have?\n",
        "Q3. What is A2 + 5?\n",
        "Q4. Who was the A3-th U.S. President?\n",
        "Q5. What is the first letter of A4's surname?\n",
        "Q6. Which of \"Mario\" and \"Luigi\" start with A5?\n",
        "Q7. Give an array of the first A3 positive integers.\n",
        "Q8. What is the second-to-last element of A7?\n",
        "Q9. Remove the letter \"u\" from A8 spelled as a word.\n",
        "Q10. Interpolate: \"One {A9} all, all {A9} one.\"\n",
        "Q11. Who wrote Don Quixote?\n",
        "Q12. What is A11's first name?\n",
        "Q13. Are A6 and A12 the same string? Yes/No.\n",
        "Q14. Repeat A13, A3 times in a row.\n",
        "Q15. Concatenate the strings A14 and A6.\n",
        "\n",
        "Use this format:\n",
        "\n",
        "Question ${number}: ${Question text}\n",
        "Rationale: Let's think step by step. First, ${write step-by-step deductions.}\n",
        "Answer: A{number} ${Answer}\n",
        "\n",
        "Repeat this format for each question.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAJbQrKot3QG",
        "outputId": "4c761ce8-d7b7-47a0-e9d5-34468e12ac6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q1. Who was the first U.S. President?\n",
            "Rationale: Let's think step by step. First, we know that the first man to walk on the moon was Neil Armstrong, so this question does not refer to the answer of the previous question.\n",
            "Rationale: We also know that the first U.S. President was George Washington.\n",
            "Answer: A1 George Washington.\n",
            "\n",
            "Q2. How many biological children did A1 have?\n",
            "Rationale: Let's think step by step. First, we know that A1 refers to George Washington, so this question refers to the answer of the previous question.\n",
            "Rationale: We also know that George Washington had no biological children.\n",
            "Answer: A2 0.\n",
            "\n",
            "Q3. What is A2 + 5?\n",
            "Rationale: Let's think step by step. First, we know that A2 refers to the answer of the previous question, which was 0.\n",
            "Rationale: We also know that 0 + 5 is 5.\n",
            "Answer: A3 5.\n",
            "\n",
            "Q4. Who was the A3-th U.S. President?\n",
            "Rationale: Let's think step by step. First, we know that A3 refers to the answer of the previous question, which was 5.\n",
            "Rationale: We also know that the 5th U.S. President was James Monroe.\n",
            "Answer: A4 James Monroe.\n",
            "\n",
            "Q5. What is the first letter of A4's surname?\n",
            "Rationale: Let's think step by step. First, we know that A4 refers to the answer of the previous question, which was James Monroe.\n",
            "Rationale: We also know that the first letter of James Monroe's surname is \"M\".\n",
            "Answer: A5 M.\n",
            "\n",
            "Q6. Which of \"Mario\" and \"Luigi\" start with A5?\n",
            "Rationale: Let's think step by step. First, we know that A5 refers to the answer of the previous question, which was \"M\".\n",
            "Rationale: We also know that the name \"Mario\" starts with \"M\".\n",
            "Answer: A6 Mario.\n",
            "\n",
            "Q7. Give an array of the first A3 positive integers.\n",
            "Rationale: Let's think step by step. First, we know that A3 refers to the answer of the third question, which was 5.\n",
            "Rationale: We also know that the first five positive integers are 1, 2, 3, 4, and 5.\n",
            "Answer: A7 [1, 2, 3, 4, 5].\n",
            "\n",
            "Q8. What is the second-to-last element of A7?\n",
            "Rationale: Let's think step by step. First, we know that A7 refers to the answer of the previous question, which was an array of the first five positive integers.\n",
            "Rationale: We also know that the second-to-last element of the array is 4.\n",
            "Answer: A8 4.\n",
            "\n",
            "Q9. Remove the letter \"u\" from A8 spelled as a word.\n",
            "Rationale: Let's think step by step. First, we know that A8 refers to the answer of the previous question, which was 4.\n",
            "Rationale: We also know that the word for 4 without the letter \"u\" is \"for\".\n",
            "Answer: A9 for.\n",
            "\n",
            "Q10. Interpolate: \"One {A9} all, all {A9} one.\"\n",
            "Rationale: Let's think step by step. First, we know that A9 refers to the answer of the previous question, which was \"for\".\n",
            "Rationale: We also know that the interpolation of \"One for all, all for one.\"\n",
            "Answer: A10 One for all, all for one.\n",
            "\n",
            "Q11. Who wrote Don Quixote?\n",
            "Rationale: Let's think step by step. First, we know that Don Quixote was written by Miguel de Cervantes.\n",
            "Answer: A11 Miguel de Cervantes.\n",
            "\n",
            "Q12. What is A11's first name?\n",
            "Rationale: Let's think step by step. First, we know that A11 refers to the answer of the previous question, which was Miguel de Cervantes.\n",
            "Rationale: We also know that Miguel de Cervantes' first name is Miguel.\n",
            "Answer: A12 Miguel.\n",
            "\n",
            "Q13. Are A6 and A12 the same string? Yes/No.\n",
            "Rationale: Let's think step by step. First, we know that A6 refers to the answer of the previous question, which was \"Mario\", and A12 refers to the answer of the previous question, which was \"Miguel\".\n",
            "Rationale: We also know that \"Mario\" and \"Miguel\" are not the same string.\n",
            "Answer: A13 No.\n",
            "\n",
            "Q14. Repeat A13, A3 times in a row.\n",
            "Rationale: Let's think step by step. First, we know that A13 refers to the answer of the previous question, which was \"No\", and A3 refers to the answer of the third question, which was 5.\n",
            "Rationale: We also know that \"No\" repeated 5 times in a row is \"NoNoNoNoNo\".\n",
            "Answer: A14 NoNoNoNoNo.\n",
            "\n",
            "Q15. Concatenate the strings A14 and A6.\n",
            "Rationale: Let's think step by step. First, we know that A14 refers to the answer of the previous question, which was \"NoNoNoNoNo\", and A6 refers to the answer of the sixth question, which was \"Mario\".\n",
            "Rationale: We also know that the concatenation of \"NoNoNoNoNo\" and \"Mario\" is \"NoNoNoNoNoMario\".\n",
            "Answer: A15 NoNoNoNoNoMario.\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(question_chain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhGA1pa_2jhx"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Pretty exciting stuff! [Langchain](https://github.com/hwchase17/langchain) makes it pretty easy to experiment with a variety of LLMs and interaction patterns.  You should check it out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bVBMrVX3dkJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
